{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094fe3f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:12.265756Z",
     "iopub.status.busy": "2025-02-13T20:02:12.265275Z",
     "iopub.status.idle": "2025-02-13T20:02:26.666996Z",
     "shell.execute_reply": "2025-02-13T20:02:26.665712Z"
    },
    "papermill": {
     "duration": 14.414788,
     "end_time": "2025-02-13T20:02:26.669711",
     "exception": false,
     "start_time": "2025-02-13T20:02:12.254923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import os\n",
    "import csv\n",
    "import argparse\n",
    "from time import time\n",
    "# Keras\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Reshape, UpSampling2D, Conv2DTranspose, GlobalAveragePooling1D, Softmax\n",
    "from keras.losses import kullback_leibler_divergence\n",
    "import keras.backend as K\n",
    "# scikit-learn\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "# Dataset helper function\n",
    "# DTC components\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, LeakyReLU, MaxPool1D, LSTM, Bidirectional, TimeDistributed, Dense, Reshape\n",
    "from keras.layers import UpSampling2D, Conv2DTranspose\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, Attention\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee83c2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:26.687890Z",
     "iopub.status.busy": "2025-02-13T20:02:26.687244Z",
     "iopub.status.idle": "2025-02-13T20:02:26.904769Z",
     "shell.execute_reply": "2025-02-13T20:02:26.903593Z"
    },
    "papermill": {
     "duration": 0.22962,
     "end_time": "2025-02-13T20:02:26.907638",
     "exception": false,
     "start_time": "2025-02-13T20:02:26.678018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "import tensorflow_addons as tfa\n",
    "from math import pi, ceil\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.losses import CategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dda032",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:26.926614Z",
     "iopub.status.busy": "2025-02-13T20:02:26.926194Z",
     "iopub.status.idle": "2025-02-13T20:02:26.938010Z",
     "shell.execute_reply": "2025-02-13T20:02:26.936777Z"
    },
    "papermill": {
     "duration": 0.024274,
     "end_time": "2025-02-13T20:02:26.940309",
     "exception": false,
     "start_time": "2025-02-13T20:02:26.916035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import SpectralClustering\n",
    "# %tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import layers\n",
    "from keras.models import Sequential,Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import time\n",
    "print(tf.__version__)\n",
    "from keras.layers import MultiHeadAttention\n",
    "from keras.layers import Dense\n",
    "import gc\n",
    "from keras.layers import concatenate\n",
    "import csv\n",
    "import math\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# import xgboost as xgb\n",
    "warnings.filterwarnings('ignore')\n",
    "# import GPy, GPyOpt\n",
    "tfkl = tf.keras.layers\n",
    "tfk = tf.keras\n",
    "\n",
    "from texttable import Texttable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe883e28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:26.958552Z",
     "iopub.status.busy": "2025-02-13T20:02:26.958136Z",
     "iopub.status.idle": "2025-02-13T20:02:26.963148Z",
     "shell.execute_reply": "2025-02-13T20:02:26.961969Z"
    },
    "papermill": {
     "duration": 0.016863,
     "end_time": "2025-02-13T20:02:26.965497",
     "exception": false,
     "start_time": "2025-02-13T20:02:26.948634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5627663d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:26.984203Z",
     "iopub.status.busy": "2025-02-13T20:02:26.983262Z",
     "iopub.status.idle": "2025-02-13T20:02:26.988603Z",
     "shell.execute_reply": "2025-02-13T20:02:26.987715Z"
    },
    "papermill": {
     "duration": 0.017168,
     "end_time": "2025-02-13T20:02:26.990837",
     "exception": false,
     "start_time": "2025-02-13T20:02:26.973669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import KLDivergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8667a01a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:27.009476Z",
     "iopub.status.busy": "2025-02-13T20:02:27.008721Z",
     "iopub.status.idle": "2025-02-13T20:02:27.021061Z",
     "shell.execute_reply": "2025-02-13T20:02:27.020012Z"
    },
    "papermill": {
     "duration": 0.023964,
     "end_time": "2025-02-13T20:02:27.023257",
     "exception": false,
     "start_time": "2025-02-13T20:02:26.999293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def autoencoder(latent_dim=8, series_len=None):\n",
    "\n",
    "    # Input\n",
    "    x = Input((None, series_len),name='input')\n",
    "    #masked_seq = layers.Masking(mask_value=0.0)(encoder_inputs)\n",
    "    # Encoder\n",
    "    encoded = LSTM(256, return_sequences=True, name=\"encoded_lstm1\")(x)\n",
    "    encoded = LSTM(128, return_sequences=True, name=\"encoded_lstm2\")(encoded)\n",
    "    attention = Attention(name=\"encoded_atten\")([encoded, encoded, encoded])  # Apply self-attention to the encoder outputs\n",
    "    merged = layers.Concatenate(axis=-1)([encoded, attention])        \n",
    "    encoded = LSTM(64, name=\"encoded_lstm3\")(merged)\n",
    "    encoded = Dense(latent_dim, name=\"encoded_dense\")(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = RepeatVector(1)(encoded)\n",
    "    decoded = LSTM(64, return_sequences=True, name=\"decoded_lstm1\")(decoded)\n",
    "    decoded = LSTM(128, return_sequences=True, name=\"decoded_lstm2\")(decoded)\n",
    "    decoded = LSTM(256, return_sequences=True, name=\"decoded_lstm3\")(decoded)\n",
    "    attention = Attention(name=\"decoded_atten\")([decoded, decoded, decoded])  # Apply self-attention to the decoder outputs\n",
    "    decoded = layers.Concatenate(axis=-1, name=\"decoded_concat\")([decoded, attention])\n",
    "    decoder_outputs = TimeDistributed(Dense(series_len), name=\"decoded_timeD\")(decoded)\n",
    "    \n",
    "    # AE model\n",
    "    autoencoder = Model(inputs=x, outputs=decoder_outputs, name='AE')\n",
    "\n",
    "    # Encoder model\n",
    "    encoder = Model(inputs=x, outputs=encoded, name='encoder')\n",
    "\n",
    "    # Create input for decoder model\n",
    "    encoded_input =Input(shape=(latent_dim,))\n",
    "    # Internal layers in decoder\n",
    "    decoded = RepeatVector(1)(encoded_input)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm1')(decoded)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm2')(decoded)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm3')(decoded)\n",
    "    attention = autoencoder.get_layer('decoded_atten')([decoded, decoded, decoded])\n",
    "    decoded = autoencoder.get_layer('decoded_concat')([decoded, attention])\n",
    "    decoder_output = autoencoder.get_layer('decoded_timeD')(decoded)\n",
    "    # Decoder model\n",
    "    decoder = Model(inputs=encoded_input, outputs=decoder_output, name='decoder')\n",
    "\n",
    "    return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7934b37f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:27.041295Z",
     "iopub.status.busy": "2025-02-13T20:02:27.040902Z",
     "iopub.status.idle": "2025-02-13T20:02:27.050498Z",
     "shell.execute_reply": "2025-02-13T20:02:27.049589Z"
    },
    "papermill": {
     "duration": 0.021232,
     "end_time": "2025-02-13T20:02:27.052867",
     "exception": false,
     "start_time": "2025-02-13T20:02:27.031635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten\n",
    "from tensorflow.keras import models\n",
    "\n",
    "def autoencoder_NN(series_len=270, latent_dim=10):\n",
    "    \"\"\"\n",
    "    Creates the encoder and decoder for the autoencoder model with shape (None, latent_dim).\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    encoder_input = Input((series_len,), name='input')\n",
    "    x = Dense(500, activation='relu', name='encode_FC1_500')(encoder_input)\n",
    "    x = Dense(500, activation='relu', name='encode_FC2_500')(x)\n",
    "    x = Dense(2000, activation='relu', name='encode_FC3_2000')(x)\n",
    "    latent_space = Dense(latent_dim, name=\"latent_space\")(x)\n",
    "\n",
    "    encoder = models.Model(inputs=encoder_input, outputs=latent_space, name=\"Encoder\")\n",
    "\n",
    "    # Decoder\n",
    "    decoder_input = Input(shape=(latent_dim,), name='decoder_input')\n",
    "    x = Dense(2000, activation='relu', name='decode_FC1_2000')(decoder_input)\n",
    "    x = Dense(500, activation='relu', name='decode_FC2_500')(x)\n",
    "    x = Dense(500, activation='relu', name='decode_FC3_500')(x)\n",
    "\n",
    "    # Add a time dimension\n",
    "    x = Dense(series_len, activation=None, name=\"reconstructed_output\")(x)\n",
    "\n",
    "    decoder = models.Model(inputs=decoder_input, outputs=x, name=\"Decoder\")\n",
    "\n",
    "    # Autoencoder\n",
    "    encoded = encoder(encoder_input)\n",
    "    decoded = decoder(encoded)\n",
    "\n",
    "    autoencoder = models.Model(inputs=encoder_input, outputs=decoded, name=\"Autoencoder\")\n",
    "\n",
    "    return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777eb395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:27.071579Z",
     "iopub.status.busy": "2025-02-13T20:02:27.071226Z",
     "iopub.status.idle": "2025-02-13T20:02:27.086317Z",
     "shell.execute_reply": "2025-02-13T20:02:27.085124Z"
    },
    "papermill": {
     "duration": 0.027745,
     "end_time": "2025-02-13T20:02:27.089019",
     "exception": false,
     "start_time": "2025-02-13T20:02:27.061274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def autoencoderNeighbour(latent_dim=8, series_len=None):\n",
    "\n",
    "    # Input\n",
    "    x = Input((None, series_len), name='input')\n",
    "    #masked_seq = layers.Masking(mask_value=0.0)(encoder_inputs)\n",
    "    # Encoder\n",
    "    encoded = LSTM(256, return_sequences=True, name='lstm_input_256')(x)\n",
    "    encoded = LSTM(128, return_sequences=True, name='lstm_input_128')(encoded)\n",
    "    attention = Attention(name='attention_input')([encoded, encoded, encoded])  # Apply self-attention to the encoder outputs\n",
    "    merged = layers.Concatenate(axis=-1)([encoded, attention])        \n",
    "    encoded = LSTM(64, name='lstm_input_64')(merged)\n",
    "    encoded = Dense(latent_dim, name='dense_input')(encoded)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x_neighbour1 = Input((None, series_len), name='Neighbour1_input')\n",
    "    x_neighbour2 = Input((None, series_len), name='Neighbour2_input')\n",
    "    \n",
    "    encoded_neighbour1 = LSTM(256, return_sequences=True, name='lstm_Neighbour1_256')(x_neighbour1)\n",
    "    encoded_neighbour2 = LSTM(256, return_sequences=True, name='lstm_Neighbour2_256')(x_neighbour2)\n",
    "    encoded_neighbour1 = LSTM(128, return_sequences=True, name='lstm_Neighbour1_128')(encoded_neighbour1)\n",
    "    encoded_neighbour2 = LSTM(128, return_sequences=True, name='lstm_Neighbour2_128')(encoded_neighbour2)\n",
    "    attention_neighbour1 = Attention(name='attention_Neighbour1')([encoded_neighbour1, encoded_neighbour2, encoded_neighbour2])  # Apply self-attention to the encoder outputs\n",
    "    attention_neighbour2 = Attention(name='attention_Neighbour2')([encoded_neighbour2, encoded_neighbour1, encoded_neighbour1])  # Apply self-attention to the encoder outputs\n",
    "    merged_neighbour1 = layers.Concatenate(axis=-1)([encoded_neighbour1, attention_neighbour1])        \n",
    "    merged_neighbour2 = layers.Concatenate(axis=-1)([encoded_neighbour2, attention_neighbour2])        \n",
    "    encoded_neighbour1 = LSTM(64, name='lstm_Neighbour1_64')(merged_neighbour1)\n",
    "    encoded_neighbour2 = LSTM(64, name='lstm_Neighbour2_64')(merged_neighbour2)\n",
    "    encoded_neighbour1 = Dense(latent_dim, name='dense_Neighbour1')(encoded_neighbour1)\n",
    "    encoded_neighbour2 = Dense(latent_dim, name='dense_Neighbour2')(encoded_neighbour2)\n",
    "\n",
    "    out_encoder_Neighbor = layers.Maximum()([encoded_neighbour1, encoded_neighbour2]) \n",
    "    # add Final encoder\n",
    "    encoded = layers.add([encoded ,out_encoder_Neighbor],name='add_inputs')\n",
    "    \n",
    "    \n",
    "    # Decoder\n",
    "    decoded = RepeatVector(1)(encoded)\n",
    "    decoded = LSTM(64, return_sequences=True, name=\"decoded_lstm1\")(decoded)\n",
    "    decoded = LSTM(128, return_sequences=True, name=\"decoded_lstm2\")(decoded)\n",
    "    decoded = LSTM(256, return_sequences=True, name=\"decoded_lstm3\")(decoded)\n",
    "    attention = Attention(name=\"decoded_atten\")([decoded, decoded, decoded])  # Apply self-attention to the decoder outputs\n",
    "    decoded = layers.Concatenate(axis=-1, name=\"decoded_concat\")([decoded, attention])\n",
    "    decoder_outputs = TimeDistributed(Dense(series_len), name=\"decoded_timeD\")(decoded)\n",
    "    \n",
    "    # AE model\n",
    "    autoencoder = Model(inputs=[x,x_neighbour1,x_neighbour2], outputs=decoder_outputs, name='AE')\n",
    "\n",
    "    # Encoder model\n",
    "    encoder = Model(inputs=[x,x_neighbour1,x_neighbour2], outputs=encoded, name='encoder')\n",
    "\n",
    "    # Create input for decoder model\n",
    "    encoded_input =Input(shape=(latent_dim,))\n",
    "    # Internal layers in decoder\n",
    "    decoded = RepeatVector(1)(encoded_input)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm1')(decoded)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm2')(decoded)\n",
    "    decoded = autoencoder.get_layer('decoded_lstm3')(decoded)\n",
    "    attention = autoencoder.get_layer('decoded_atten')([decoded, decoded, decoded])\n",
    "    decoded = autoencoder.get_layer('decoded_concat')([decoded, attention])\n",
    "    decoder_output = autoencoder.get_layer('decoded_timeD')(decoded)\n",
    "    # Decoder model\n",
    "    decoder = Model(inputs=encoded_input, outputs=decoder_output, name='decoder')\n",
    "\n",
    "    return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd1c8ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:27.107672Z",
     "iopub.status.busy": "2025-02-13T20:02:27.107308Z",
     "iopub.status.idle": "2025-02-13T20:02:27.118145Z",
     "shell.execute_reply": "2025-02-13T20:02:27.116995Z"
    },
    "papermill": {
     "duration": 0.023067,
     "end_time": "2025-02-13T20:02:27.120503",
     "exception": false,
     "start_time": "2025-02-13T20:02:27.097436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def autoencoder_NN_Neighbour(latent_dim=10, series_len=None):\n",
    "\n",
    "    # Input\n",
    "    encoder_input = Input((series_len,), name='input')\n",
    "    x = Dense(500, activation='relu', name='encode_FC1_500')(encoder_input)\n",
    "    x = Dense(500, activation='relu', name='encode_FC2_500')(x)\n",
    "    x = Dense(2000, activation='relu', name='encode_FC3_2000')(x)\n",
    "    latent_space = Dense(latent_dim, name=\"latent_space\")(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x_neighbour1 = Input((series_len,), name='Neighbour1_input')\n",
    "    x_neighbour2 = Input((series_len,), name='Neighbour2_input')\n",
    "    \n",
    "    encoded_neighbour1 = Dense(500, activation='relu', name='encode_Neighbour1_FC1_500')(x_neighbour1)\n",
    "    encoded_neighbour2 = Dense(500, activation='relu', name='encode_Neighbour2_FC1_500')(x_neighbour2)\n",
    "    encoded_neighbour1 = Dense(500, activation='relu', name='encode_Neighbour1_FC2_500')(encoded_neighbour1)\n",
    "    encoded_neighbour2 = Dense(500, activation='relu', name='encode_Neighbour2_FC2_500')(encoded_neighbour2)        \n",
    "    encoded_neighbour1 = Dense(2000, activation='relu', name='encode_Neighbour1_FC3_2000')(encoded_neighbour1)\n",
    "    encoded_neighbour2 = Dense(2000, activation='relu', name='encode_Neighbour2_FC3_2000')(encoded_neighbour2)\n",
    "    encoded_neighbour1 = Dense(latent_dim, name=\"latent_Neighbour1_space\")(encoded_neighbour1)\n",
    "    encoded_neighbour2 = Dense(latent_dim, name=\"latent_Neighbour2_space\")(encoded_neighbour2)\n",
    "\n",
    "    out_encoder_Neighbor = layers.Maximum()([encoded_neighbour1, encoded_neighbour2]) \n",
    "    # add Final encoder\n",
    "    encoded = layers.add([latent_space ,out_encoder_Neighbor],name='add_inputs')\n",
    "    \n",
    "    \n",
    "    # Decoder\n",
    "    decoder_input = Input(shape=(latent_dim,), name='decoder_input')\n",
    "    x = Dense(2000, activation='relu', name='decode_FC1_2000')(decoder_input)\n",
    "    x = Dense(500, activation='relu', name='decode_FC2_500')(x)\n",
    "    x = Dense(500, activation='relu', name='decode_FC3_500')(x)\n",
    "    # Add a time dimension\n",
    "    x = Dense(series_len, activation=None, name=\"reconstructed_output\")(x)\n",
    "\n",
    "    \n",
    "    encoder = models.Model(inputs=[encoder_input,x_neighbour1,x_neighbour2], outputs=encoded, name=\"Encoder\")\n",
    "    decoder = models.Model(inputs=decoder_input, outputs=x, name=\"Decoder\")\n",
    "    encoded = encoder([encoder_input, x_neighbour1, x_neighbour2])\n",
    "    decoded = decoder(encoded)\n",
    "    autoencoder = models.Model(inputs=[encoder_input,x_neighbour1,x_neighbour2], outputs=decoded, name=\"Autoencoder\")\n",
    "    return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321fff4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:27.140233Z",
     "iopub.status.busy": "2025-02-13T20:02:27.139800Z",
     "iopub.status.idle": "2025-02-13T20:02:27.151711Z",
     "shell.execute_reply": "2025-02-13T20:02:27.150517Z"
    },
    "papermill": {
     "duration": 0.024362,
     "end_time": "2025-02-13T20:02:27.154086",
     "exception": false,
     "start_time": "2025-02-13T20:02:27.129724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eucl(x, y):\n",
    "    \"\"\"\n",
    "    Euclidean distance between two multivariate time series given as arrays of shape (timesteps, dim)\n",
    "    \"\"\"\n",
    "    d = np.sqrt(np.sum(np.square(x - y), axis=0))\n",
    "    return np.sum(d)\n",
    "\n",
    "\n",
    "def cid(x, y):\n",
    "    \"\"\"\n",
    "    Complexity-Invariant Distance (CID) between two multivariate time series given as arrays of shape (timesteps, dim)\n",
    "    Reference: Batista, Wang & Keogh (2011). A Complexity-Invariant Distance Measure for Time Series. https://doi.org/10.1137/1.9781611972818.60\n",
    "    \"\"\"\n",
    "    assert(len(x.shape) == 2 and x.shape == y.shape)  # time series must have same length and dimensionality\n",
    "    ce_x = np.sqrt(np.sum(np.square(np.diff(x, axis=0)), axis=0) + 1e-9)\n",
    "    ce_y = np.sqrt(np.sum(np.square(np.diff(y, axis=0)), axis=0) + 1e-9)\n",
    "    d = np.sqrt(np.sum(np.square(x - y), axis=0)) * np.divide(np.maximum(ce_x, ce_y), np.minimum(ce_x, ce_y))\n",
    "    return np.sum(d)\n",
    "\n",
    "\n",
    "def cor(x, y):\n",
    "    \"\"\"\n",
    "    Correlation-based distance (COR) between two multivariate time series given as arrays of shape (timesteps, dim)\n",
    "    \"\"\"\n",
    "    scaler = TimeSeriesScalerMeanVariance()\n",
    "    x_norm = scaler.fit_transform(x)\n",
    "    y_norm = scaler.fit_transform(y)\n",
    "    pcc = np.mean(x_norm * y_norm)  # Pearson correlation coefficients\n",
    "    d = np.sqrt(2.0 * (1.0 - pcc + 1e-9))  # correlation-based similarities\n",
    "    return np.sum(d)\n",
    "\n",
    "\n",
    "def acf(x, y):\n",
    "    \"\"\"\n",
    "    Autocorrelation-based distance (ACF) between two multivariate time series given as arrays of shape (timesteps, dim)\n",
    "    Computes a linearly weighted euclidean distance between the autocorrelation coefficients of the input time series.\n",
    "    Reference: Galeano & Pena (2000). Multivariate Analysis in Vector Time Series.\n",
    "    \"\"\"\n",
    "    assert (len(x.shape) == 2 and x.shape == y.shape)  # time series must have same length and dimensionality\n",
    "    x_acf = np.apply_along_axis(lambda z: stattools.acf(z, nlags=z.shape[0]), 0, x)\n",
    "    y_acf = np.apply_along_axis(lambda z: stattools.acf(z, nlags=z.shape[0]), 0, y)\n",
    "    weights = np.linspace(1.0, 0.0, x.shape[0])\n",
    "    d = np.sqrt(np.sum(np.expand_dims(weights, axis=1) * np.square(x_acf - y_acf), axis=0))\n",
    "    return np.sum(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bccaa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:27.172993Z",
     "iopub.status.busy": "2025-02-13T20:02:27.172624Z",
     "iopub.status.idle": "2025-02-13T20:02:27.188129Z",
     "shell.execute_reply": "2025-02-13T20:02:27.187009Z"
    },
    "papermill": {
     "duration": 0.028399,
     "end_time": "2025-02-13T20:02:27.190768",
     "exception": false,
     "start_time": "2025-02-13T20:02:27.162369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import precision_score, recall_score, adjusted_rand_score, fowlkes_mallows_score\n",
    "\n",
    "def cluster_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate unsupervised clustering accuracy, precision, recall, Rand Index, and Fowlkes–Mallows Index.\n",
    "\n",
    "    # Arguments\n",
    "        y_true: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "\n",
    "    # Return\n",
    "        A dictionary with accuracy, precision, recall, Rand Index, and FMI.\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    \n",
    "    # Determine the mapping using Hungarian algorithm\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    \n",
    "    # Map predicted labels to true labels based on the Hungarian algorithm\n",
    "    label_mapping = {row: col for row, col in zip(row_ind, col_ind)}\n",
    "    y_pred_mapped = np.array([label_mapping[label] for label in y_pred])\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = w[row_ind, col_ind].sum() * 1.0 / y_pred.size\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = precision_score(y_true, y_pred_mapped, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred_mapped, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Calculate Rand Index\n",
    "    rand_index = adjusted_rand_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate Fowlkes–Mallows Index\n",
    "    fmi = fowlkes_mallows_score(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'rand_index': rand_index,\n",
    "        'fowlkes_mallows_index': fmi\n",
    "    }\n",
    "\n",
    "def cluster_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate unsupervised clustering accuracy. Requires scikit-learn installed\n",
    "\n",
    "    # Arguments\n",
    "        y_true: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    return w[row_ind, col_ind].sum() * 1.0 / y_pred.size\n",
    "\n",
    "\n",
    "def cluster_purity(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering purity\n",
    "\n",
    "    # Arguments\n",
    "        y_true: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "\n",
    "    # Return\n",
    "        purity, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    label_mapping = w.argmax(axis=1)\n",
    "    y_pred_voted = y_pred.copy()\n",
    "    for i in range(y_pred.size):\n",
    "        y_pred_voted[i] = label_mapping[y_pred[i]]\n",
    "    return metrics.accuracy_score(y_pred_voted, y_true)\n",
    "\n",
    "\n",
    "def roc_auc(y_true, q_pred, n_classes):\n",
    "    \"\"\"\n",
    "    Calculate area under ROC curve (ROC AUC)\n",
    "    WARNING: DO NOT USE, MAY CONTAIN ERRORS\n",
    "    TODO: CHECK IT!\n",
    "\n",
    "    # Arguments\n",
    "        y_true: true labels, numpy.array with shape `(n_samples,)`\n",
    "        q_pred: predicted probabilities, numpy.array with shape `(n_samples,)`\n",
    "\n",
    "    # Return\n",
    "        ROC AUC score, in [0,1]\n",
    "    \"\"\"\n",
    "    if n_classes == 2:  # binary ROC AUC\n",
    "        auc = max(metrics.roc_auc_score(y_true, q_pred[:, 1]), metrics.roc_auc_score(y_true, q_pred[:, 0]))\n",
    "    else:  # micro-averaged ROC AUC (multiclass)\n",
    "        fpr, tpr, _ = metrics.roc_curve(label_binarize(y_true, classes=np.unique(y_true)).ravel(), q_pred.ravel())\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dfac7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:27.209571Z",
     "iopub.status.busy": "2025-02-13T20:02:27.209212Z",
     "iopub.status.idle": "2025-02-13T20:02:27.221322Z",
     "shell.execute_reply": "2025-02-13T20:02:27.220132Z"
    },
    "papermill": {
     "duration": 0.024304,
     "end_time": "2025-02-13T20:02:27.223624",
     "exception": false,
     "start_time": "2025-02-13T20:02:27.199320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: degrees of freedom parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, FeatureWeight, weights=None, a=1.0, b=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.FeatureWeight = FeatureWeight\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "         Measure the similarity between embedded point z_i and centroid µ_j.\n",
    "                 q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n",
    "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "                 (i.e., a soft assignment)\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "\n",
    "        global FeatureWeight\n",
    "        q = (1 + self.a * (K.sum((self.FeatureWeight) * K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) ** (self.b))) ** (-1)  \n",
    "        FeatureWeight = K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters) * K.expand_dims(q**2, axis=2)  ,axis=0)\n",
    "        FeatureWeight = K.transpose(K.transpose(FeatureWeight) / K.sum(FeatureWeight, axis=1))\n",
    "                   \n",
    "        return q \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6ad3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:27.242212Z",
     "iopub.status.busy": "2025-02-13T20:02:27.241825Z",
     "iopub.status.idle": "2025-02-13T20:02:27.259136Z",
     "shell.execute_reply": "2025-02-13T20:02:27.257970Z"
    },
    "papermill": {
     "duration": 0.029739,
     "end_time": "2025-02-13T20:02:27.261520",
     "exception": false,
     "start_time": "2025-02-13T20:02:27.231781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TSClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, timesteps, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "        dist_metric: distance metric between sequences used in similarity kernel ('eucl', 'cir', 'cor' or 'acf').\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(n_samples, timesteps, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, dist_metric='eucl', **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(TSClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.dist_metric = dist_metric\n",
    "        self.initial_weights = weights\n",
    "        self.clusters = None\n",
    "        self.built = False\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_shape[1]))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_shape[1]), initializer='glorot_uniform', name='cluster_centers')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    '''def call(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Student t-distribution kernel, probability of assigning encoded sequence i to cluster k.\n",
    "            q_{ik} = (1 + dist(z_i, m_k)^2)^{-1} / normalization.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: encoded input sequences, shape=(n_samples, timesteps, n_features)\n",
    "        Return:\n",
    "            q: soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        if self.dist_metric == 'eucl':\n",
    "            distance = K.sum(K.sqrt(K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=1)), axis=-1)\n",
    "        elif self.dist_metric == 'cid':\n",
    "            ce_x = K.sqrt(K.sum(K.square(inputs[:, 1:, :] - inputs[:, :-1, :]), axis=1))  # shape (n_samples, n_features)\n",
    "            ce_w = K.sqrt(K.sum(K.square(self.clusters[:, 1:, :] - self.clusters[:, :-1, :]), axis=1))  # shape (n_clusters, n_features)\n",
    "            ce = K.maximum(K.expand_dims(ce_x, axis=1), ce_w) / K.minimum(K.expand_dims(ce_x, axis=1), ce_w)  # shape (n_samples, n_clusters, n_features)\n",
    "            ed = K.sqrt(K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2))  # shape (n_samples, n_clusters, n_features)\n",
    "            distance = K.sum(ed * ce, axis=-1)  # shape (n_samples, n_clusters)\n",
    "        elif self.dist_metric == 'cor':\n",
    "            inputs_norm = (inputs - K.expand_dims(K.mean(inputs, axis=1), axis=1)) / K.expand_dims(K.std(inputs, axis=1), axis=1)  # shape (n_samples, timesteps, n_features)\n",
    "            clusters_norm = (self.clusters - K.expand_dims(K.mean(self.clusters, axis=1), axis=1)) / K.expand_dims(K.std(self.clusters, axis=1), axis=1)  # shape (n_clusters, timesteps, n_features)\n",
    "            pcc = K.mean(K.expand_dims(inputs_norm, axis=1) * clusters_norm, axis=2)  # Pearson correlation coefficients\n",
    "            distance = K.sum(K.sqrt(2.0 * (1.0 - pcc)), axis=-1)  # correlation-based similarities, shape (n_samples, n_clusters)\n",
    "        elif self.dist_metric == 'acf':\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise ValueError('Available distances are eucl, cid, cor and acf!')\n",
    "        q = 1.0 / (1.0 + K.square(distance) / self.alpha)\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q'''\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Student t-distribution kernel, probability of assigning encoded sequence i to cluster k.\n",
    "            q_{ik} = (1 + dist(z_i, m_k)^2)^{-1} / normalization.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: encoded input sequences, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        inputs_expanded = K.expand_dims(inputs, axis=1)  # shape=(n_samples, 1, n_features)\n",
    "        clusters_expanded = K.expand_dims(self.clusters, axis=0)  # shape=(1, n_clusters, n_features)\n",
    "\n",
    "        if self.dist_metric == 'eucl':\n",
    "            distance = K.sqrt(K.sum(K.square(inputs_expanded - clusters_expanded), axis=-1))\n",
    "        elif self.dist_metric == 'cid':\n",
    "            ce_x = K.sqrt(K.sum(K.square(inputs_expanded - K.expand_dims(inputs[:, :-1], axis=2)), axis=-1))  # shape (n_samples, timesteps)\n",
    "            ce_w = K.sqrt(K.sum(K.square(clusters_expanded - K.expand_dims(self.clusters[:, :-1], axis=1)), axis=-1))  # shape (n_clusters, timesteps)\n",
    "            ce = K.maximum(K.expand_dims(ce_x, axis=1), ce_w) / K.minimum(K.expand_dims(ce_x, axis=1), ce_w)  # shape (n_samples, n_clusters, timesteps)\n",
    "            ed = K.sqrt(K.sum(K.square(inputs_expanded - clusters_expanded), axis=-1))  # shape (n_samples, n_clusters)\n",
    "            distance = K.sum(ed * ce, axis=-1)  # shape (n_samples, n_clusters)\n",
    "        elif self.dist_metric == 'cor':\n",
    "            inputs_norm = (inputs - K.mean(inputs, axis=1, keepdims=True)) / K.std(inputs, axis=1, keepdims=True)  # shape (n_samples, n_features)\n",
    "            clusters_norm = (self.clusters - K.mean(self.clusters, axis=1, keepdims=True)) / K.std(self.clusters, axis=1, keepdims=True)  # shape (n_clusters, n_features)\n",
    "            pcc = K.mean(K.expand_dims(inputs_norm, axis=1) * K.expand_dims(clusters_norm, axis=0), axis=-1)  # Pearson correlation coefficients\n",
    "            distance = K.sqrt(2.0 * (1.0 - pcc))  # correlation-based similarities, shape (n_samples, n_clusters)\n",
    "        elif self.dist_metric == 'acf':\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise ValueError('Available distances are eucl, cid, cor and acf!')\n",
    "\n",
    "        q = 1.0 / (1.0 + K.square(distance) / self.alpha)\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 3\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters, 'dist_metric': self.dist_metric}\n",
    "        base_config = super(TSClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a036f69e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:27.280035Z",
     "iopub.status.busy": "2025-02-13T20:02:27.279672Z",
     "iopub.status.idle": "2025-02-13T20:02:27.286930Z",
     "shell.execute_reply": "2025-02-13T20:02:27.285520Z"
    },
    "papermill": {
     "duration": 0.019384,
     "end_time": "2025-02-13T20:02:27.289281",
     "exception": false,
     "start_time": "2025-02-13T20:02:27.269897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to save the class\n",
    "import json\n",
    "import numpy as np\n",
    "def save_dtc_class(dtc, path=None):\n",
    "    # Save the class variables\n",
    "    #with open(\"dtc_state.json\", \"w\") as state_file:\n",
    "    #    json.dump(dtc.__dict__, state_file)\n",
    "\n",
    "    # Save the Keras models\n",
    "    if dtc.model:\n",
    "        dtc.model.save_weights(\"dtc_model.h5\")\n",
    "    if dtc.autoencoder:\n",
    "        dtc.autoencoder.save_weights(\"dtc_autoencoder.h5\")\n",
    "        dtc.autoencoderNeighbour.save_weights(\"dtc_autoencoder_neighbour.h5\")\n",
    "    if dtc.encoder:\n",
    "        dtc.encoder.save_weights(\"dtc_encoder.h5\")\n",
    "        dtc.encoderNeighbour.save_weights(\"dtc_encoder_neighbour.h5\")\n",
    "    if dtc.decoder:\n",
    "        dtc.decoder.save_weights(\"dtc_decoder.h5\")\n",
    "        dtc.decoderNeighbour.save_weights(\"dtc_decoder_neighbour.h5\")\n",
    "    print(\"DTC class saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d758d326",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:27.307478Z",
     "iopub.status.busy": "2025-02-13T20:02:27.307047Z",
     "iopub.status.idle": "2025-02-13T20:02:28.162308Z",
     "shell.execute_reply": "2025-02-13T20:02:28.161076Z"
    },
    "papermill": {
     "duration": 0.867451,
     "end_time": "2025-02-13T20:02:28.164958",
     "exception": false,
     "start_time": "2025-02-13T20:02:27.297507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial.distance as distance\n",
    "import scipy.sparse.linalg as linalg\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "#from fastdtw import fastdtw\n",
    "\n",
    "def complexity_estimate(series):\n",
    "    \"\"\"Computes the complexity estimate of a time series.\"\"\"\n",
    "    return np.sqrt(np.sum(np.diff(series) ** 2))\n",
    "\n",
    "def cid_distance(x, y):\n",
    "    \"\"\"Computes the Complexity Invariant Distance (CID) between two time series.\"\"\"\n",
    "    euclidean_dist = np.linalg.norm(x - y)\n",
    "    ce_x = complexity_estimate(x)\n",
    "    ce_y = complexity_estimate(y)\n",
    "    return euclidean_dist * ((ce_x + ce_y) / 2)\n",
    "\n",
    "def compute_similarity_matrix(data, metric='euclidean', sigma=1.0):\n",
    "    \"\"\"\n",
    "    Compute the adjacency (similarity) matrix using various distance metrics.\n",
    "    \"\"\"\n",
    "    if metric in ['euclidean', 'cityblock', 'cosine', 'chebyshev']:\n",
    "        pairwise_distances = distance.pdist(data, metric=metric)  # Compute pairwise distances\n",
    "    elif metric == 'dtw':\n",
    "        pairwise_distances = np.array([[fastdtw(data[i], data[j])[0] for j in range(len(data))] for i in range(len(data))])\n",
    "    elif metric == 'cid':\n",
    "        pairwise_distances = np.array([[cid_distance(data[i], data[j]) for j in range(len(data))] for i in range(len(data))])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported metric: {metric}\")\n",
    "    \n",
    "    if metric in ['euclidean', 'cityblock', 'cosine', 'chebyshev']:\n",
    "        adjacency_matrix = np.exp(-pairwise_distances ** 2 / (2. * sigma ** 2)) if metric == 'euclidean' else 1 / (1 + pairwise_distances)  # Apply transformation\n",
    "        adjacency_matrix = distance.squareform(adjacency_matrix)  # Convert to square form\n",
    "    else:\n",
    "        adjacency_matrix = 1 / (1 + pairwise_distances)  # Normalize distances\n",
    "    \n",
    "    np.fill_diagonal(adjacency_matrix, 0)  # Remove self-loops\n",
    "    return adjacency_matrix\n",
    "\n",
    "def compute_graph_laplacian(adjacency_matrix, normalized=True):\n",
    "    \"\"\"\n",
    "    Compute the graph Laplacian matrix.\n",
    "    \"\"\"\n",
    "    degree_matrix = np.diag(np.sum(adjacency_matrix, axis=1))\n",
    "    laplacian = degree_matrix - adjacency_matrix  # Unnormalized Laplacian\n",
    "    \n",
    "    if normalized:\n",
    "        d_inv_sqrt = np.diag(1.0 / np.sqrt(np.diag(degree_matrix) + 1e-8))  # Avoid division by zero\n",
    "        laplacian = np.matmul(np.matmul(d_inv_sqrt, laplacian), d_inv_sqrt)  # L_sym = D^(-1/2) * L * D^(-1/2)\n",
    "    \n",
    "    return laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8166869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:28.183824Z",
     "iopub.status.busy": "2025-02-13T20:02:28.183363Z",
     "iopub.status.idle": "2025-02-13T20:02:28.257278Z",
     "shell.execute_reply": "2025-02-13T20:02:28.256383Z"
    },
    "papermill": {
     "duration": 0.086468,
     "end_time": "2025-02-13T20:02:28.259787",
     "exception": false,
     "start_time": "2025-02-13T20:02:28.173319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DTC:\n",
    "    def __init__(self, n_clusters, input_dim, timesteps,\n",
    "                 alpha=1.0, dist_metric='eucl', cluster_init='kmeans', heatmap=False):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.input_dim = None\n",
    "        self.timesteps = 791\n",
    "        self.latent_shape = 10\n",
    "        self.alpha = alpha\n",
    "        self.dist_metric = dist_metric\n",
    "        self.cluster_init = cluster_init\n",
    "        self.heatmap = heatmap\n",
    "        self.pretrained = False\n",
    "        self.alpha2 = 0.8\n",
    "        self.learning_rate = None\n",
    "        self.optimizer = keras.optimizers.Adam()\n",
    "        self.model = self.autoencoder = self.encoder = self.decoder = self.predmodel =  None\n",
    "        self.autoencoderNeighbour = self.encoderNeighbour = self.decoderNeighbour =  None\n",
    "        self.FeatureWeight = None\n",
    "        self.a = 1.0\n",
    "        self.b = 1.0\n",
    "        self.initial_weights = None\n",
    "        self.a_b = [1.93, 0.79]\n",
    "        self.KNN = None\n",
    "        self.b1 = -0.9\n",
    "        self.b2 = self.b1/2\n",
    "\n",
    "        \n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Create DTC model\n",
    "        \"\"\"\n",
    "        # Create AE models\n",
    "        self.FeatureWeight = np.ones((self.n_clusters, self.latent_shape),dtype='float32')/self.latent_shape\n",
    "        self.autoencoder, self.encoder, self.decoder = autoencoder_NN(series_len=144)\n",
    "        self.autoencoderNeighbour , self.encoderNeighbour , self.decoderNeighbour = autoencoder_NN_Neighbour(series_len=144)\n",
    "        clustering_layer = TSClusteringLayer(self.n_clusters,\n",
    "                                             alpha=self.alpha,\n",
    "                                             dist_metric=self.dist_metric,\n",
    "                                             name='TSClustering')(self.encoderNeighbour.output)\n",
    "        # Create DTC model\n",
    "        self.model = Model(inputs=[self.autoencoderNeighbour.input],\n",
    "                               outputs=[self.autoencoderNeighbour.output, clustering_layer])\n",
    "        self.KNN = NearestNeighbors(n_neighbors=3,metric='euclidean')\n",
    "\n",
    "    @property\n",
    "    def cluster_centers_(self):\n",
    "        \"\"\"\n",
    "        Returns cluster centers\n",
    "        \"\"\"\n",
    "        return self.model.get_layer(name='TSClustering').get_weights()[0]\n",
    "    \n",
    "    def reconstruction_loss(self, data, reconstruction):\n",
    "        # Mean squared error over each sample in the batch\n",
    "        mse_loss = keras.losses.mean_squared_error(data, reconstruction)  # Shape: (267,)\n",
    "        \n",
    "        # Mean loss across the entire batch\n",
    "        reconstruction_loss = tf.reduce_mean(mse_loss)  # Single scalar value\n",
    "        return reconstruction_loss\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def weighted_kld(loss_weight):\n",
    "        \"\"\"\n",
    "        Custom KL-divergence loss with a variable weight parameter\n",
    "        \"\"\"\n",
    "        def loss(y_true, y_pred):\n",
    "            return loss_weight * kullback_leibler_divergence(y_true, y_pred)\n",
    "        return loss\n",
    "    \n",
    "    def loss_kld(self, y_true, y_pred):\n",
    "        return kullback_leibler_divergence(y_true, y_pred)\n",
    "    \n",
    "    \n",
    "    def loss_kld(self, y_true, y_pred):\n",
    "        return kullback_leibler_divergence(y_true, y_pred)\n",
    "    \n",
    "    def categorical_cross_entropy(self, y_true, y_pred):\n",
    "        cce = CategoricalCrossentropy()\n",
    "        return cce(y_true, y_pred)\n",
    "        \n",
    "    def on_epoch_end(self, epoch):\n",
    "        \"\"\"\n",
    "        Update heatmap loss weight on epoch end\n",
    "        \"\"\"\n",
    "        if epoch > self.finetune_heatmap_at_epoch:\n",
    "            K.set_value(self.heatmap_loss_weight, self.final_heatmap_loss_weight)\n",
    "\n",
    "    def compile(self, gamma, optimizer, initial_heatmap_loss_weight=None, final_heatmap_loss_weight=None):\n",
    "        \"\"\"\n",
    "        Compile DTC model\n",
    "\n",
    "        # Arguments\n",
    "            gamma: coefficient of TS clustering loss\n",
    "            optimizer: optimization algorithm\n",
    "            initial_heatmap_loss_weight (optional): initial weight of heatmap loss vs clustering loss\n",
    "            final_heatmap_loss_weight (optional): final weight of heatmap loss vs clustering loss (heatmap finetuning)\n",
    "        \"\"\"\n",
    "        if self.heatmap:\n",
    "            self.initial_heatmap_loss_weight = initial_heatmap_loss_weight\n",
    "            self.final_heatmap_loss_weight = final_heatmap_loss_weight\n",
    "            self.heatmap_loss_weight = K.variable(self.initial_heatmap_loss_weight)\n",
    "            self.model.compile(loss=['mse', DTC.weighted_kld(1.0 - self.heatmap_loss_weight), DTC.weighted_kld(self.heatmap_loss_weight)],\n",
    "                               loss_weights=[1.0, gamma, gamma],\n",
    "                               optimizer=optimizer)\n",
    "        else:\n",
    "            self.model.compile(loss=['mse', 'CategoricalCrossentropy'],\n",
    "                               loss_weights=[1.0, gamma],\n",
    "                               optimizer=optimizer)\n",
    "\n",
    "    def load_weights(self, weights_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained weights of DTC model\n",
    "\n",
    "        # Arguments\n",
    "            weight_path: path to weights file (.h5)\n",
    "        \"\"\"\n",
    "        self.model.load_weights(weights_path)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_ae_weights(self, ae_weights_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained weights of AE\n",
    "\n",
    "        # Arguments\n",
    "            ae_weight_path: path to weights file (.h5)\n",
    "        \"\"\"\n",
    "        self.autoencoder.load_weights(ae_weights_path)\n",
    "        self.pretrained = True\n",
    "\n",
    "\n",
    "    def manifold_loss(self, data, reconstruction, features):\n",
    "\n",
    "        # Choose the similarity measure (example: Pearson)\n",
    "        adj_matrix = compute_similarity_matrix(data, metric='cid')\n",
    "        L = compute_graph_laplacian(adj_matrix, normalized=True)\n",
    "\n",
    "        L = tf.convert_to_tensor(L, dtype=tf.float32)  # Convert sparse matrix to dense before TF tensor\n",
    "        \"\"\"Custom loss function combining reconstruction loss with whole-dataset manifold regularization.\"\"\"\n",
    "        \n",
    "        reconstruction_loss = self.reconstruction_loss(data, reconstruction)\n",
    "        \n",
    "        \n",
    "        # Compute manifold regularization using the entire dataset\n",
    "        graph_loss_latent = tf.linalg.trace(tf.matmul(tf.matmul(tf.transpose(features), L), features))\n",
    "        graph_loss_reconstruction = tf.linalg.trace(tf.matmul(tf.matmul(tf.transpose(reconstruction), L), reconstruction))\n",
    "        print(\"Graph Loss: \", graph_loss_latent)\n",
    "        return reconstruction_loss +  0.01 * graph_loss_latent # + 0.01 * graph_loss_reconstruction # Weighted sum\n",
    "        \n",
    "    def load_ae_neighbour_weights(self):\n",
    "        input = self.autoencoder.get_layer(name='input').get_weights()\n",
    "        encoder_0 = self.autoencoder.get_layer(name='Encoder').get_layer(name='encode_FC1_500').get_weights()\n",
    "        encoder_1 = self.autoencoder.get_layer(name='Encoder').get_layer(name='encode_FC2_500').get_weights()\n",
    "        encoder_2 = self.autoencoder.get_layer(name='Encoder').get_layer(name='encode_FC3_2000').get_weights()\n",
    "        dense_encode = self.autoencoder.get_layer(name='Encoder').get_layer(name=\"latent_space\").get_weights()\n",
    "        decoder_0 = self.autoencoder.get_layer(name='Decoder').get_layer(name='decode_FC1_2000').get_weights()\n",
    "        decoder_1 = self.autoencoder.get_layer(name='Decoder').get_layer(name='decode_FC2_500').get_weights()\n",
    "        decoder_2 = self.autoencoder.get_layer(name='Decoder').get_layer(name='decode_FC3_500').get_weights()\n",
    "        decoder_timeD = self.autoencoder.get_layer(name='Decoder').get_layer(name=\"reconstructed_output\").get_weights()\n",
    "\n",
    "        # Image weight\n",
    "        self.autoencoderNeighbour.get_layer(name='Encoder').get_layer(name='input').set_weights(input)\n",
    "        self.autoencoderNeighbour.get_layer(name='Encoder').get_layer(name='encode_FC1_500').set_weights(encoder_0)\n",
    "        self.autoencoderNeighbour.get_layer(name='Encoder').get_layer(name='encode_FC2_500').set_weights(encoder_1)\n",
    "        self.autoencoderNeighbour.get_layer(name='Encoder').get_layer(name='encode_FC3_2000').set_weights(encoder_2)\n",
    "        self.autoencoderNeighbour.get_layer(name='Encoder').get_layer(name='latent_space').set_weights(dense_encode)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        self.autoencoderNeighbour.get_layer(name='Decoder').get_layer(name='decode_FC1_2000').set_weights(decoder_0)\n",
    "        self.autoencoderNeighbour.get_layer(name='Decoder').get_layer(name='decode_FC2_500').set_weights(decoder_1)\n",
    "        self.autoencoderNeighbour.get_layer(name='Decoder').get_layer(name='decode_FC3_500').set_weights(decoder_2)\n",
    "\n",
    "        self.autoencoderNeighbour.get_layer(name='Encoder').get_layer(name='encode_Neighbour1_FC1_500').set_weights(encoder_0)\n",
    "        self.autoencoderNeighbour.get_layer(name='Encoder').get_layer(name='encode_Neighbour1_FC2_500').set_weights(encoder_1)\n",
    "        self.autoencoderNeighbour.get_layer(name='Encoder').get_layer(name='encode_Neighbour1_FC3_2000').set_weights(encoder_2)\n",
    "        \n",
    "        self.autoencoderNeighbour.get_layer(name='Encoder').get_layer(name='encode_Neighbour2_FC1_500').set_weights(encoder_0)\n",
    "        self.autoencoderNeighbour.get_layer(name='Encoder').get_layer(name='encode_Neighbour2_FC2_500').set_weights(encoder_1)\n",
    "        self.autoencoderNeighbour.get_layer(name='Encoder').get_layer(name='encode_Neighbour2_FC3_2000').set_weights(encoder_2)\n",
    "\n",
    "        self.autoencoderNeighbour.get_layer(name='Neighbour1_input').set_weights(input)\n",
    "        self.autoencoderNeighbour.get_layer(name='Neighbour2_input').set_weights(input)\n",
    "\n",
    "        self.encoderNeighbour.get_layer(name='input').set_weights(input)\n",
    "        self.encoderNeighbour.get_layer(name='encode_FC1_500').set_weights(encoder_0)\n",
    "        self.encoderNeighbour.get_layer(name='encode_FC2_500').set_weights(encoder_1)\n",
    "        self.encoderNeighbour.get_layer(name='encode_FC3_2000').set_weights(encoder_2)\n",
    "        \n",
    "        self.encoderNeighbour.get_layer(name='encode_Neighbour1_FC1_500').set_weights(encoder_0)\n",
    "        self.encoderNeighbour.get_layer(name='encode_Neighbour1_FC2_500').set_weights(encoder_1)\n",
    "        self.encoderNeighbour.get_layer(name='encode_Neighbour1_FC3_2000').set_weights(encoder_2)\n",
    "        \n",
    "        self.encoderNeighbour.get_layer(name='encode_Neighbour2_FC1_500').set_weights(encoder_0)\n",
    "        self.encoderNeighbour.get_layer(name='encode_Neighbour2_FC2_500').set_weights(encoder_1)\n",
    "        self.encoderNeighbour.get_layer(name='encode_Neighbour2_FC3_2000').set_weights(encoder_2)\n",
    "\n",
    "        self.encoderNeighbour.get_layer(name='Neighbour1_input').set_weights(input)\n",
    "        self.encoderNeighbour.get_layer(name='Neighbour2_input').set_weights(input)\n",
    "\n",
    "    def dist(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Compute distance between two multivariate time series using chosen distance metric\n",
    "\n",
    "        # Arguments\n",
    "            x1: first input (np array)\n",
    "            x2: second input (np array)\n",
    "        # Return\n",
    "            distance\n",
    "        \"\"\"\n",
    "        if self.dist_metric == 'eucl':\n",
    "            return tsdistances.eucl(x1, x2)\n",
    "        elif self.dist_metric == 'cid':\n",
    "            return tsdistances.cid(x1, x2)\n",
    "        elif self.dist_metric == 'cor':\n",
    "            return tsdistances.cor(x1, x2)\n",
    "        elif self.dist_metric == 'acf':\n",
    "            return tsdistances.acf(x1, x2)\n",
    "        else:\n",
    "            raise ValueError('Available distances are eucl, cid, cor and acf!')\n",
    "\n",
    "    def init_cluster_weights(self, X):\n",
    "        \"\"\"\n",
    "        Initialize with complete-linkage hierarchical clustering or k-means.\n",
    "\n",
    "        # Arguments\n",
    "            X: numpy array containing training set or batch\n",
    "        \"\"\"\n",
    "        assert(self.cluster_init in ['hierarchical', 'kmeans'])\n",
    "        print('Initializing cluster...')\n",
    "\n",
    "        features = self.encode(X)\n",
    "\n",
    "        if self.cluster_init == 'hierarchical':\n",
    "            if self.dist_metric == 'eucl':  # use AgglomerativeClustering off-the-shelf\n",
    "                hc = AgglomerativeClustering(n_clusters=self.n_clusters,\n",
    "                                             affinity='euclidean',\n",
    "                                             linkage='complete').fit(features.reshape(features.shape[0], -1))\n",
    "            else:  # compute distance matrix using dist\n",
    "                d = np.zeros((features.shape[0], features.shape[0]))\n",
    "                for i in range(features.shape[0]):\n",
    "                    for j in range(i):\n",
    "                        d[i, j] = d[j, i] = self.dist(features[i], features[j])\n",
    "                hc = AgglomerativeClustering(n_clusters=self.n_clusters,\n",
    "                                             affinity='precomputed',\n",
    "                                             linkage='complete').fit(d)\n",
    "            # compute centroid\n",
    "            cluster_centers = np.array([features[hc.labels_ == c].mean(axis=0) for c in range(self.n_clusters)])\n",
    "        elif self.cluster_init == 'kmeans':\n",
    "            # fit k-means on flattened features\n",
    "            km = KMeans(n_clusters=self.n_clusters, n_init=10).fit(features.reshape(features.shape[0], -1))\n",
    "            cluster_centers = km.cluster_centers_.reshape(self.n_clusters, features.shape[1])\n",
    "\n",
    "        self.model.get_layer(name='TSClustering').set_weights([cluster_centers])\n",
    "        print('Done!')\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encoding function. Extract latent features from hidden layer\n",
    "\n",
    "        # Arguments\n",
    "            x: data point\n",
    "        # Return\n",
    "            encoded (latent) data point\n",
    "        \"\"\"\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        \"\"\"\n",
    "        Decoding function. Decodes encoded sequence from latent space.\n",
    "\n",
    "        # Arguments\n",
    "            x: encoded (latent) data point\n",
    "        # Return\n",
    "            decoded data point\n",
    "        \"\"\"\n",
    "        return self.decoder.predict(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict cluster assignment.\n",
    "\n",
    "        \"\"\"\n",
    "        q = self.model.predict(x, verbose=0)[1]\n",
    "        return q.argmax(axis=1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def target_distribution(q):  # target distribution p which enhances the discrimination of soft label q\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def predict_heatmap(self, x):\n",
    "        \"\"\"\n",
    "        Produces TS clustering heatmap from input sequence.\n",
    "\n",
    "        # Arguments\n",
    "            x: data point\n",
    "        # Return\n",
    "            heatmap\n",
    "        \"\"\"\n",
    "        return self.heatmap_model.predict(x, verbose=0)\n",
    "\n",
    "    def pretrain(self, X,\n",
    "                 optimizer='adam',\n",
    "                 epochs=10,\n",
    "                 batch_size=64,\n",
    "                 save_dir='results/tmp',\n",
    "                 verbose=1):\n",
    "        \"\"\"\n",
    "        Pre-train the autoencoder using only MSE reconstruction loss\n",
    "        Saves weights in h5 format.\n",
    "\n",
    "        # Arguments\n",
    "            X: training set\n",
    "            optimizer: optimization algorithm\n",
    "            epochs: number of pre-training epochs\n",
    "            batch_size: training batch size\n",
    "            save_dir: path to existing directory where weights will be saved\n",
    "        \"\"\"\n",
    "        print('Pretraining...')\n",
    "        self.autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        # Begin pretraining\n",
    "        #t0 = time()\n",
    "        #self.autoencoder.fit(X, X, batch_size=batch_size, epochs=epochs)\n",
    "        #print('Pretraining time: ', time() - t0)\n",
    "        early_stopping = EarlyStopping(\n",
    "        monitor='loss',  # Monitor the training loss\n",
    "        patience=3,  # Number of epochs with no improvement before stopping\n",
    "        restore_best_weights=True,  # Restore the best weights\n",
    "        verbose=verbose\n",
    "        )\n",
    "\n",
    "        # Begin pretraining\n",
    "        self.autoencoder.fit(\n",
    "            X, X,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=[early_stopping],  # Add EarlyStopping to callbacks\n",
    "            verbose=verbose\n",
    "        )\n",
    "        self.autoencoder.save_weights('{}/ae_weights-epoch{}.h5'.format(save_dir, epochs))\n",
    "        print('Pretrained weights are saved to {}/ae_weights-epoch{}.h5'.format(save_dir, epochs))\n",
    "        self.pretrained = True\n",
    "\n",
    "    def manifold_loss_pretrain(self, data, reconstruction):\n",
    "\n",
    "        # Choose the similarity measure (example: Pearson)\n",
    "        adj_matrix = compute_similarity_matrix(data, metric='cid')\n",
    "        L = compute_graph_laplacian(adj_matrix, normalized=True)\n",
    "\n",
    "        L = tf.convert_to_tensor(L, dtype=tf.float32)  # Convert sparse matrix to dense before TF tensor\n",
    "        \"\"\"Custom loss function combining reconstruction loss with whole-dataset manifold regularization.\"\"\"\n",
    "        \n",
    "        reconstruction_loss = self.reconstruction_loss(data, reconstruction)\n",
    "        \n",
    "        \n",
    "        # Compute manifold regularization using the entire dataset\n",
    "        graph_loss_reconstruction = tf.linalg.trace(tf.matmul(tf.matmul(tf.transpose(reconstruction), L), reconstruction))\n",
    "        return reconstruction_loss + 0.01 * graph_loss_reconstruction # Weighted sum\n",
    "\n",
    "    \n",
    "    def custom_fit(self, X_train, X_val=None, batch_size=10, epochs=200, patience=3, verbose=1, save_dir=\"/kaggle/working/\"):\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, X_train)).batch(batch_size)\n",
    "        \n",
    "        best_train_loss = np.inf\n",
    "        wait = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "            train_loss = []\n",
    "            \n",
    "            for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    reconstructed = self.autoencoder(x_batch, training=True)\n",
    "                    loss = self.manifold_loss_pretrain(y_batch, reconstructed)\n",
    "                grads = tape.gradient(loss, self.autoencoder.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, self.autoencoder.trainable_variables))\n",
    "                train_loss.append(loss.numpy())\n",
    "                \n",
    "            avg_train_loss = np.mean(train_loss)\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "            \n",
    "            if avg_train_loss < best_train_loss:\n",
    "                best_train_loss = avg_train_loss\n",
    "                wait = 0\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    print(\"Early stopping triggered on training loss.\")\n",
    "                    break\n",
    "\n",
    "\n",
    "        self.autoencoder.save_weights('{}/ae_weights-epoch{}.h5'.format(save_dir, epochs))\n",
    "        print('Pretrained weights are saved to {}/ae_weights-epoch{}.h5'.format(save_dir, epochs))\n",
    "        self.pretrained = True\n",
    "    \n",
    "    def calculate_silhouette_score(self, data, labels):\n",
    "        \"\"\"\n",
    "        Calculate the silhouette score for clustering results.\n",
    "\n",
    "        Parameters:\n",
    "        - data: np.array, the dataset with shape (n_samples, n_features)\n",
    "        - labels: np.array, the cluster assignments for each sample\n",
    "\n",
    "        Returns:\n",
    "        - silhouette: float, the silhouette score ranging from -1 to 1\n",
    "        \"\"\"\n",
    "        if len(np.unique(labels)) > 1:  # Silhouette score requires at least 2 clusters\n",
    "            return silhouette_score(data, labels)\n",
    "        else:\n",
    "            return -1  # Return -1 if only one cluster exists\n",
    "        \n",
    "        \n",
    "    \n",
    "    def calculate_davies_bouldin_index(self, data, labels):\n",
    "        \"\"\"\n",
    "        Calculate the Davies-Bouldin index for clustering results.\n",
    "\n",
    "        Parameters:\n",
    "        - data: np.array, the dataset with shape (n_samples, n_features)\n",
    "        - labels: np.array, the cluster assignments for each sample\n",
    "\n",
    "        Returns:\n",
    "        - db_index: float, the Davies-Bouldin index (lower is better)\n",
    "        \"\"\"\n",
    "        if len(np.unique(labels)) > 1:  # DBI requires at least 2 clusters\n",
    "            return davies_bouldin_score(data, labels)\n",
    "        else:\n",
    "            return float('inf')  # Return infinity if only one cluster exists\n",
    "    \n",
    "    \n",
    "    \n",
    "    def calculate_calinski_harabasz_index(self, data, labels):\n",
    "        \"\"\"\n",
    "        Calculate the Calinski-Harabasz index for clustering results.\n",
    "\n",
    "        Parameters:\n",
    "        - data: np.array, the dataset with shape (n_samples, n_features)\n",
    "        - labels: np.array, the cluster assignments for each sample\n",
    "\n",
    "        Returns:\n",
    "        - ch_index: float, the Calinski-Harabasz index (higher is better)\n",
    "        \"\"\"\n",
    "        if len(np.unique(labels)) > 1:  # Calinski-Harabasz requires at least 2 clusters\n",
    "            return calinski_harabasz_score(data, labels)\n",
    "        else:\n",
    "            return 0  # Return 0 if only one cluster exists\n",
    "    \n",
    "    \n",
    "    \n",
    "    def save_metrics(self, metrics, file_path='metrics.csv'):\n",
    "        \"\"\"\n",
    "        Save training metrics to a CSV file.\n",
    "\n",
    "        # Arguments\n",
    "            metrics: List of dictionaries containing metrics for each epoch\n",
    "            file_path: Path to the CSV file where metrics will be saved\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(metrics)\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f'Metrics saved to {file_path}')\n",
    "        \n",
    "        \n",
    "    def adjusted_rand_index(self, true_labels, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the Adjusted Rand Index (ARI) between true and predicted labels.\n",
    "        \n",
    "        # Arguments\n",
    "            true_labels: True cluster labels\n",
    "            predicted_labels: Predicted cluster labels\n",
    "        \n",
    "        # Return\n",
    "            ARI score\n",
    "        \"\"\"\n",
    "        print(y_pred)\n",
    "        return adjusted_rand_score(true_labels, y_pred)\n",
    "    \n",
    "    def normalized_mutual_info(self, true_labels, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the Normalized Mutual Information (NMI) between true and predicted labels.\n",
    "        \n",
    "        # Arguments\n",
    "            true_labels: True cluster labels\n",
    "            predicted_labels: Predicted cluster labels\n",
    "        \n",
    "        # Return\n",
    "            NMI score\n",
    "        \"\"\"\n",
    "        print(y_pred)\n",
    "        return normalized_mutual_info_score(true_labels, y_pred)\n",
    "\n",
    "\n",
    "    def calculate_cid(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculate Complexity-Invariant Distance (CID) between two time series.\n",
    "        \"\"\"\n",
    "        # Ensure the input time series are 1D\n",
    "        x = np.squeeze(x)\n",
    "        y = np.squeeze(y)\n",
    "        \n",
    "        # Compute Euclidean distance\n",
    "        ed = np.sqrt(np.sum((x - y) ** 2))\n",
    "        \n",
    "        # Compute complexity of x\n",
    "        complexity_x = np.sqrt(np.sum(np.diff(x) ** 2))\n",
    "        \n",
    "        # Compute complexity of y\n",
    "        complexity_y = np.sqrt(np.sum(np.diff(y) ** 2))\n",
    "        \n",
    "        # Compute complexity ratio\n",
    "        complexity_ratio = max(complexity_x, complexity_y) / min(complexity_x, complexity_y)\n",
    "        \n",
    "        # Calculate CID\n",
    "        cid = ed * complexity_ratio\n",
    "        return cid\n",
    "\n",
    "    def calculate_cid_matrix(self, data):\n",
    "        \"\"\"\n",
    "        Calculate a CID distance matrix for all pairs of time series in a dataset.\n",
    "        \n",
    "        Parameters:\n",
    "            data (np.ndarray): 2D array where each row is a time series (n_series, series_length).\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Distance matrix of shape (n_series, n_series).\n",
    "        \"\"\"\n",
    "        n_series = data.shape[0]\n",
    "        cid_matrix = np.zeros((n_series, n_series))\n",
    "        \n",
    "        for i in range(n_series):\n",
    "            for j in range(i, n_series):  # Avoid duplicate calculations\n",
    "                cid = self.calculate_cid(data[i], data[j])\n",
    "                cid_matrix[i, j] = cid\n",
    "                cid_matrix[j, i] = cid  # Symmetric matrix\n",
    "        \n",
    "        return cid_matrix\n",
    "\n",
    "    def calculate_cid_silhouette_score(self, distance_matrix, labels):\n",
    "        \"\"\"\n",
    "        Calculate silhouette score using a precomputed distance matrix.\n",
    "    \n",
    "        Parameters:\n",
    "            distance_matrix (np.ndarray): Precomputed distance matrix (n_samples, n_samples).\n",
    "            labels (np.ndarray): Cluster labels for each point (n_samples,).\n",
    "    \n",
    "        Returns:\n",
    "            float: Silhouette score.\n",
    "        \"\"\"\n",
    "        # Use the precomputed distance matrix with the 'precomputed' metric\n",
    "        score = silhouette_score(distance_matrix, labels, metric=\"precomputed\")\n",
    "        return score\n",
    "\n",
    "\n",
    "    def evaluate_clustering(self, data, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Evaluate the clustering performance using ARI and NMI scores.\n",
    "        \n",
    "        # Arguments\n",
    "            X: Data to be clustered\n",
    "            y_true: True cluster labels\n",
    "        \n",
    "        # Return\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute ARI and NMI\n",
    "        ari_score = self.adjusted_rand_index(y_true, y_pred)\n",
    "        nmi_score = self.normalized_mutual_info(y_true, y_pred)\n",
    "        cid_distance = self.calculate_cid_matrix(data)\n",
    "        silhouette_cid = self.calculate_cid_silhouette_score(cid_distance, y_pred)\n",
    "        accuracy = cluster_acc(y_true, y_pred)\n",
    "        purity = cluster_purity(y_true, y_pred)\n",
    "        metrics = cluster_metrics(y_true, y_pred)\n",
    "        \n",
    "        print(f\"Adjusted Rand Index (ARI): {ari_score:.4f}\")\n",
    "        print(f\"Normalized Mutual Information (NMI): {nmi_score:.4f}\")\n",
    "        print(f\"CID Silhouette Score: \", silhouette_cid)\n",
    "        print(f\"Accuracy: \", accuracy)\n",
    "        print(f\"Cluster Purity: \", purity)\n",
    "        print(f\"Accuracy: \", metrics['accuracy'])\n",
    "        print(f\"Precision: \", metrics['precision'])\n",
    "        print(f\"Recall: \", metrics['recall'])\n",
    "        print(f\"Rand Index: \", metrics['rand_index'])\n",
    "        print(f\"Fowlkes Mallows Index: \", metrics['fowlkes_mallows_index'])\n",
    "    \n",
    "    def fit(self, X_train, y_train=None, X_val=None, y_val=None,\n",
    "        epochs=50,\n",
    "        eval_epochs=10,\n",
    "        save_epochs=10,\n",
    "        batch_size=64,\n",
    "        tol=0.001,\n",
    "        patience=5,\n",
    "        finetune_heatmap_at_epoch=8,\n",
    "        save_dir='results/tmp',\n",
    "        mutation_rate=0.01,\n",
    "        crossover_rate=0.5,\n",
    "        num_iterations=5):\n",
    "            \n",
    "            index = 0\n",
    "            if not self.pretrained:\n",
    "                print('Autoencoder was not pre-trained!')\n",
    "            print('Training for {} epochs.\\nEvaluating every {} and saving model every {} epochs.'.format(epochs, eval_epochs, save_epochs))\n",
    "            best_val_error = float('inf')\n",
    "            index_array = np.arange(X_train.shape[0])\n",
    "            for epoch in range(epochs):\n",
    "                if epoch == 0:\n",
    "                    q = self.model.predict([X_train, X_train, X_train])[1]\n",
    "                    extract = Model(inputs = self.model.input, outputs = self.model.get_layer('add_inputs').output)\n",
    "                    z = extract.predict([X_train, X_train, X_train])\n",
    "                else:\n",
    "                    self.KNN.fit(z)\n",
    "                    _,Neighbors_list=self.KNN.kneighbors(z)\n",
    "                    q = self.model.predict([X_train,X_train[Neighbors_list[:,1]], X_train[Neighbors_list[:,2]]])[1]\n",
    "\n",
    "                p = DTC.target_distribution(q)\n",
    "                p_pred = p.argmax(axis=1)\n",
    "                new_clustering = p_pred.copy()\n",
    "                reliableindex, _ = np.unique(np.unique(np.where(np.sort(q,axis=1)[:,-1] - np.sort(q,axis=1)[:,-2]>=self.b2), np.where(np.max(q,axis=1)>=self.b1)),\n",
    "                           np.unique(np.where(np.sort(p,axis=1)[:,-1] - np.sort(p,axis=1)[:,-2]>=self.b2), np.where(np.max(p,axis=1)>=self.b1)))\n",
    "\n",
    "                print('Number of reliable samples:', len(q[reliableindex]) )\n",
    "                print('Number of unreliable samples:', len(q)-len(q[reliableindex]) )\n",
    "\n",
    "                Number_Unreliable = len(q)-len(q[reliableindex])\n",
    "                Number_reliable = len(q[reliableindex])\n",
    "                with tf.GradientTape() as tape:\n",
    "                    idx = index_array[index:reliableindex.shape[0]]\n",
    "                    self.KNN.fit(z[reliableindex[idx]])\n",
    "                    _,Neighbors_list=self.KNN.kneighbors(z[reliableindex[idx]])\n",
    "                    outputs = self.model([X_train[reliableindex[idx]], X_train[Neighbors_list[:,1]], X_train[Neighbors_list[:,2]]], training=True)\n",
    "                    feature_extraction = Model(inputs = self.model.input, outputs = self.model.get_layer('add_inputs').output)\n",
    "                    features = feature_extraction.predict([X_train[reliableindex[idx]], X_train[Neighbors_list[:,1]], X_train[Neighbors_list[:,2]]])\n",
    "                    reconstruction_output, clustering_output = outputs\n",
    "                    reconstruction_loss = self.manifold_loss(X_train[reliableindex[idx]], reconstruction_output, features)\n",
    "                    clustering_loss = self.categorical_cross_entropy(p[reliableindex[idx]], clustering_output)\n",
    "\n",
    "                    total_loss = tf.reduce_sum(reconstruction_loss) + tf.reduce_sum(clustering_loss)\n",
    "                    total_loss = tf.reduce_sum(total_loss)\n",
    "\n",
    "\n",
    "                feature_extraction = Model(inputs = self.model.input, outputs = self.model.get_layer('add_inputs').output)\n",
    "                features = feature_extraction.predict([X_train, X_train, X_train])\n",
    "                np.save(f\"features_{epoch}.npy\", features)\n",
    "                print(f\"Features saved to 'features_{epoch}.npy'\")\n",
    "                grads = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "                print(\"Reconstruction Loss: \", reconstruction_loss)\n",
    "                print(\"Clustering Loss: \", clustering_loss)\n",
    "                num_elements = len(reliableindex[idx])\n",
    "                n1 = Neighbors_list[:,1]\n",
    "                n1 = n1[:num_elements]\n",
    "                n2 = Neighbors_list[:,2]\n",
    "                n2 = n2[:num_elements]\n",
    "                z[idx] = extract.predict([X_train[reliableindex[idx]], X_train[n1], X_train[n2]])\n",
    "                self.evaluate_clustering(X_train, np.array(p_pred), y_train)\n",
    "                print(\"********************************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab3f473",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:28.278064Z",
     "iopub.status.busy": "2025-02-13T20:02:28.277711Z",
     "iopub.status.idle": "2025-02-13T20:02:28.358212Z",
     "shell.execute_reply": "2025-02-13T20:02:28.356884Z"
    },
    "papermill": {
     "duration": 0.092247,
     "end_time": "2025-02-13T20:02:28.360689",
     "exception": false,
     "start_time": "2025-02-13T20:02:28.268442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(train_path, test_path):\n",
    "    # Load the train dataset\n",
    "    train_df = pd.read_csv(train_path, delim_whitespace=True, header=None)\n",
    "    \n",
    "    # Load the test dataset\n",
    "    test_df = pd.read_csv(test_path, delim_whitespace=True, header=None)\n",
    "    \n",
    "    # Extract features and labels for the train set\n",
    "    X_train = train_df.iloc[:, 1:]  # All rows, all columns except the first one\n",
    "    y_train = train_df.iloc[:, 0]   # All rows, first column\n",
    "    \n",
    "    # Extract features and labels for the test set\n",
    "    X_test = test_df.iloc[:, 1:]  # All rows, all columns except the first one\n",
    "    y_test = test_df.iloc[:, 0]   # All rows, first column\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Example usage\n",
    "train_path = '/kaggle/input/timeseriesclassificationdataset/PowerCons/PowerCons_TRAIN.txt'\n",
    "test_path = '/kaggle/input/timeseriesclassificationdataset/PowerCons/PowerCons_TEST.txt'\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(train_path, test_path)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "print(\"Train features shape:\", X_train.shape)\n",
    "print(\"Train labels shape:\", y_train.shape)\n",
    "print(\"Test features shape:\", X_test.shape)\n",
    "print(\"Test labels shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6de043",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:28.403737Z",
     "iopub.status.busy": "2025-02-13T20:02:28.402506Z",
     "iopub.status.idle": "2025-02-13T20:02:28.409502Z",
     "shell.execute_reply": "2025-02-13T20:02:28.408199Z"
    },
    "papermill": {
     "duration": 0.019089,
     "end_time": "2025-02-13T20:02:28.412178",
     "exception": false,
     "start_time": "2025-02-13T20:02:28.393089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"X train shape: \",X_train.shape)\n",
    "print(\"X test shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903fc68e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:28.430610Z",
     "iopub.status.busy": "2025-02-13T20:02:28.429953Z",
     "iopub.status.idle": "2025-02-13T20:02:28.448380Z",
     "shell.execute_reply": "2025-02-13T20:02:28.447342Z"
    },
    "papermill": {
     "duration": 0.0306,
     "end_time": "2025-02-13T20:02:28.451003",
     "exception": false,
     "start_time": "2025-02-13T20:02:28.420403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "dtc = DTC(n_clusters=3,input_dim=X_train.shape[-1], timesteps=X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf1fada",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:28.469373Z",
     "iopub.status.busy": "2025-02-13T20:02:28.468973Z",
     "iopub.status.idle": "2025-02-13T20:02:29.750676Z",
     "shell.execute_reply": "2025-02-13T20:02:29.749249Z"
    },
    "papermill": {
     "duration": 1.30169,
     "end_time": "2025-02-13T20:02:29.761037",
     "exception": false,
     "start_time": "2025-02-13T20:02:28.459347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = 'adam'\n",
    "dtc.initialize()\n",
    "dtc.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba2720a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:29.785817Z",
     "iopub.status.busy": "2025-02-13T20:02:29.785440Z",
     "iopub.status.idle": "2025-02-13T20:02:29.807823Z",
     "shell.execute_reply": "2025-02-13T20:02:29.806475Z"
    },
    "papermill": {
     "duration": 0.038067,
     "end_time": "2025-02-13T20:02:29.810730",
     "exception": false,
     "start_time": "2025-02-13T20:02:29.772663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtc.compile(gamma=1.0, optimizer=optimizer, initial_heatmap_loss_weight=0.1,\n",
    "                final_heatmap_loss_weight=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7043f0f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:02:29.870007Z",
     "iopub.status.busy": "2025-02-13T20:02:29.869616Z",
     "iopub.status.idle": "2025-02-13T20:03:10.323744Z",
     "shell.execute_reply": "2025-02-13T20:03:10.322193Z"
    },
    "papermill": {
     "duration": 40.471995,
     "end_time": "2025-02-13T20:03:10.326501",
     "exception": false,
     "start_time": "2025-02-13T20:02:29.854506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtc.custom_fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae95b90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:03:10.353378Z",
     "iopub.status.busy": "2025-02-13T20:03:10.353001Z",
     "iopub.status.idle": "2025-02-13T20:03:10.391723Z",
     "shell.execute_reply": "2025-02-13T20:03:10.390444Z"
    },
    "papermill": {
     "duration": 0.055406,
     "end_time": "2025-02-13T20:03:10.394352",
     "exception": false,
     "start_time": "2025-02-13T20:03:10.338946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtc.load_ae_neighbour_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354fec9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:03:10.485313Z",
     "iopub.status.busy": "2025-02-13T20:03:10.484917Z",
     "iopub.status.idle": "2025-02-13T20:03:10.795074Z",
     "shell.execute_reply": "2025-02-13T20:03:10.794182Z"
    },
    "papermill": {
     "duration": 0.391543,
     "end_time": "2025-02-13T20:03:10.798030",
     "exception": false,
     "start_time": "2025-02-13T20:03:10.406487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize clusters\n",
    "dtc.init_cluster_weights(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81915b05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:03:10.826681Z",
     "iopub.status.busy": "2025-02-13T20:03:10.826282Z",
     "iopub.status.idle": "2025-02-13T20:05:33.075299Z",
     "shell.execute_reply": "2025-02-13T20:05:33.073972Z"
    },
    "papermill": {
     "duration": 142.265187,
     "end_time": "2025-02-13T20:05:33.077874",
     "exception": false,
     "start_time": "2025-02-13T20:03:10.812687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit model\n",
    "#t0 = time()\n",
    "dtc.fit(X_train, y_train=y_train-1, X_val=None, y_val=None)\n",
    "#print('Training time: ', (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860ad991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:05:33.194510Z",
     "iopub.status.busy": "2025-02-13T20:05:33.194092Z",
     "iopub.status.idle": "2025-02-13T20:05:33.590328Z",
     "shell.execute_reply": "2025-02-13T20:05:33.589050Z"
    },
    "papermill": {
     "duration": 0.457487,
     "end_time": "2025-02-13T20:05:33.592869",
     "exception": false,
     "start_time": "2025-02-13T20:05:33.135382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "print('Performance (Test)')\n",
    "results = {}\n",
    "q = dtc.model.predict([X_test, X_test, X_test])[1]\n",
    "y_pred = q.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef9a7f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:05:33.712458Z",
     "iopub.status.busy": "2025-02-13T20:05:33.712024Z",
     "iopub.status.idle": "2025-02-13T20:05:34.224327Z",
     "shell.execute_reply": "2025-02-13T20:05:34.222962Z"
    },
    "papermill": {
     "duration": 0.574422,
     "end_time": "2025-02-13T20:05:34.226649",
     "exception": false,
     "start_time": "2025-02-13T20:05:33.652227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtc.evaluate_clustering(X_test, y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b125e605",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T20:05:34.350454Z",
     "iopub.status.busy": "2025-02-13T20:05:34.349195Z",
     "iopub.status.idle": "2025-02-13T20:05:34.629695Z",
     "shell.execute_reply": "2025-02-13T20:05:34.628387Z"
    },
    "papermill": {
     "duration": 0.345673,
     "end_time": "2025-02-13T20:05:34.632071",
     "exception": false,
     "start_time": "2025-02-13T20:05:34.286398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_dtc_class(dtc, path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74716f61",
   "metadata": {
    "papermill": {
     "duration": 0.058689,
     "end_time": "2025-02-13T20:05:34.748968",
     "exception": false,
     "start_time": "2025-02-13T20:05:34.690279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb87ab2",
   "metadata": {
    "papermill": {
     "duration": 0.058377,
     "end_time": "2025-02-13T20:05:34.868907",
     "exception": false,
     "start_time": "2025-02-13T20:05:34.810530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3743351,
     "sourceId": 6479400,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3792584,
     "sourceId": 6564428,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4520850,
     "sourceId": 7735771,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4454423,
     "sourceId": 9326749,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5664474,
     "sourceId": 9345875,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 146550165,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 146550185,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 199643187,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 210.316118,
   "end_time": "2025-02-13T20:05:38.342791",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-13T20:02:08.026673",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
